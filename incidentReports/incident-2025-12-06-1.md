# Incident: 2025-12-06 11-50-00 MDT

## Summary

> [!NOTE]
> Write a summary of the incident in a few sentences. Include what happened, why, the severity of the incident, and how long the impact lasted.

```md
Between the hours of 8:00 am MDT and 11:52 am MDT on 12/6/2025, several users, around 3 each hour, encountered errors with ordering pizzas. The event was triggered by a bug initiated by the pizza service chaos testing between 8:00 am and 11:50 am. The bug initiated by the pizza service contained malicious content that made the order pizza endpoint fail.

A bug in this code caused every pizza order to fail. The event was detected by Grafana logging. The team started working on the event by 11:50. This high-severity incident affected {100%} of users.
```

## Detection

> [!NOTE]
> When did the team detect the incident? How did they know it was happening? How could we improve time-to-detection? Consider: How would we have cut that time by half?

```md
This incident was detected when the site developer, Braden Bledsoe noticed failures in the Grafana logging for pizza order endpoint failures.

The pizza-ordering alerts were not triggered because they were set up incorrectly, and the alerts monitored different things. This is why the detection is through personal, visual analysis of the metrics and logging shown on the pizza dashboard within Grafana.

Alerts will be made within the pizza order purchases, and failures view so that alerts will be triggered when failures are above 3 every time the metrics are sent, along with when pizza purchases drop below 2. Since I have continually been running a script, this will help alert me if no pizza purchases were occurring and if there were nothing but failures, or if there is a higher failure rate than what it should be. This will be set up by Braden Bledsoe so that if there is a future issue or bug with ordering pizzas, Braden will be notified even sooner, and therefore can help resolve issues related to the system quickly.
```

## Impact

> [!NOTE]
> Describe how the incident impacted internal and external users during the incident. Include how many support cases were raised.

```md
For 3hrs 50 minutes between 8:00 am MDT and 11:50 am MDT on 12/6/25, our users experienced pizza ordering failures.

This incident affected 10 customers, 100% OF SYSTEM USERS, who experienced pizza ordering failures. No support cases were raised.
```

## Timeline

> [!NOTE]
> Detail the incident timeline. We recommend using UTC to standardize for time zones.
> Include any notable lead-up events, any starts of activity, the first known impact, and escalations. Note any decisions or changes made, and when the incident ended, along with any post-impact events of note.

```md
All times are UTC.

-   _1:00_ - Monitoring of metrics started. Braden started a script to simulate traffic
-   _4:50_ - By visual observation of metrics in Grafana, Braden noticed endpoint errors with the api/order endpoint.
-   _4:51_ - Braden performed manual testing within his actual site to confirm observed metric errors. Through this testing, he confirmed there really was an error.
-   _4:52_ - Braden resolved the error by looking at the logging made in Grafana for the endpoint and clicking on the link that basically said to go to in order to resolve the error.
-   _4:53_ - Braden went to the appropriate URL and resolved the error.
-   _4:53_ - Everything resumed as normal.
```

## Response

> [!NOTE]
> Who responded to the incident? When did they respond, and what did they do? Note any delays or obstacles to responding.

```md
After choosing to recheck his system, Braden Bledsoe came online in Grafana at 11:50 MDT.
```

## Root cause

> [!NOTE]
> Note the final root cause of the incident, the thing identified that needs to change in order to prevent this class of incident from happening again.

```md
A bug in the order api request led to failed pizza orders.
```

## Resolution

> [!NOTE]
> Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.
> Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

```md
To resolve the error and bug, Braden looked at the failed logging for that api request, and then there was a link to click on that seemed promising, in order to resolve the error. He clicked on that link, and the new page said the issue was resolved.

From there, Braden checked his logs once again and noticed 200 status codes for his api/order endpoint. He then did manual testing to confirm those results. He was then able to actually order a pizza once again and confirmed the bug was resolved.
```

## Prevention

> [!NOTE]
> Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

```md
This same root cause did not invoke any other incidents.
```

## Action items

> [!NOTE]
> Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.

```md
1. Grafana logging and metrics to analyze system traffic
2. Alerts set in place within Grafana to notify the DevOps team (Braden Bledsoe for now)
3. The DevOps team should then analyze data and perform manual testing to confirm such errors and have a greater idea of what could be causing it.
4. Within 5 minutes, the root cause of the error should be found.
5. Within 30 minutes, all errors should be resolved.
```
